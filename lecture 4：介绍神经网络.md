# lecture 4：介绍神经网络

![image-20230715113106589](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230715113106589.png)

## 梯度求解

### 计算图：

计算图是一个很重要的概念，回想一下，我们现在有了一个函数LOSS，它的输入是我们标记好的tag与输入的图片，而输出则是代表误差大小的一组数据。我们要做的是让他自己找到让误差达到最小参数W，这就是一个给定函数求某一位置的最值的问题，只不过在问题中参数由一维的`x`向更高维拓展了，在一维我们求解给定给函数的最值会用到导数，而在多维中我们求解就用到梯度。

![image-20230726111131627](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230726111131627.png)

何为**梯度**：梯度的方向是函数值增加最快的方向，即上山最快的方向。我们要求最小值，则需要找到下山最快的方向。

为什么要使用计算树呢？梯度就是多维层面的导数，具体就是每一个变量的偏导数。那么想一想，在一维只有一个变量`x`时对于某些复杂式子求导就很难受，而所有的可到函数都是建立在基础函数的嵌套之上的，我们需要找到一个简单有效，程序化的方法来求出导数。所以这就出现了计算树。

计算树是在正向由内向外，由小到大的求解函数各个部分的值，再通过反向右外包内，由大到小的方式传播来由上游的值来确定下游的导数。计算树的好处是无论参与的式子多么复杂，我们都可以将它程序化的一步步变成一个可分解的算法，以便计算机能替我们进行计算。

#### 反向传播：

**拓展到多维**：雅可比矩阵



**神经网络：本质就是一个计算图**

对于前面的我们熟悉的$$f(x)=Wx$$的例子来说，这是最简单的一个线性函数，那么再这之中一组输入数据仅仅经过一次的计算就可以得到一个最终的结果。而我们发现这一的拟合效果其实不是很好，我们想到了再复合一个非线性的函数，就像$$f=W_2max(0,W_1x)$$这样，那么这就是一个两层的神经网络。而前面举得例子其实也是神经网络，只不过它是一个一层的神经网络。有一个输入与一个输出，而这个两层的神经网络会根据权重值$$W_2$$来对第一层得到的数据进行一个更优化的权重分配，从而可以达到更优的状态。

**激活函数**

![image-20230715181731809](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230715181731809.png)







![image-20230715183126335](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230715183126335.png)