# lecture 3:loss函数与最优化

## loss：



上一节我们研究了线性分类器，但是如何找到一个方法来调试好我们的参数W呢？首先我们需要引入一个评价体系来确定什么样的W是好的，什么样的W是不好的。这个用于评价的函数就是loss。

先看一下我们现在有什么:我们有线性分类器的函数.
$$
S=f(x_i,W)
$$


## SVM loss


$$
L_i=\sum max(0,S_j-S_{y_j}+k)
$$

$$
loss=\sum_i L_i
$$

解释一下这个公式: i代表输入的某张图片的序号。j代表在在一张图片中不同的类别。

`k`是一个常数，代表我们设定的限制条件范围，如果正确类的预测分数比其他类的预测分数高k，那么我们即可认为它对于误差loss所带来的影响因子为0.

## 交叉熵loss

交叉熵是经过几步规范后后得到的一个离散的分布率，把分数通过一定的变换来变成概率。

而交叉熵的loss通过对正确的类的预测概率进行函数变化：
$$
这里的一个用例是：loss=-log(P(s_{y_i}|s))
$$
用-log的好处是将误差给缩放的很大、很明显。这样我们比较、判断以及消除误差的时候变会很清晰。

![image-20230714145636031](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230714145636031.png)

## 正则化：

在了解正则化这个概念以前，我们应该先了解过拟合。

### 过拟合（overfitting）

什么是过拟合呢？就是太钻牛角尖，为了寻求在一组数据上的拟合程度最佳而生成出来不具有普适性的拟合函数。

![img](https://pic2.zhimg.com/80/v2-b4ccd2c27516e24531f956a85296f6b5_720w.webp)

像这个图中的图三就是过拟合了，这个拟合函数极力让每一个点都可以从图像当中穿过而拟合除一个复杂多项式。但是这个多项式显然不是我们要的结果，这种好心办坏事的情况就是过拟合。

**奥卡姆剃刀**：这个原理称为“**如无必要，勿增实体**”，即“**简单有效原理**”。它虽然是由英国英国十三世纪的逻辑学家提出的哲学上的理论，但是毫无疑问，我们这个世界的一切都是符合某种哲学的。这句话天天赢

复杂性惩罚：我们在高次项前面加了一个系数`θ`，当`θ`充分小时，便可以消除告辞项所带来的影响。这就是复杂性惩罚。

## 有了loss之后再去寻找W

我们前面研究了这么多都是在loss上做文章，而我们引入loss实际上是为了去寻找W。如何寻找出最好的一组W数据呢？

**下山**：沿着梯度的反方向下降，是最快到达山下的途径。

![image-20230714152214888](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230714152214888.png)

**超参数**：自己可以设定的参数。

**学习率**：也叫步长，是指在下山时具体每次一步走多大，到数据梯度下降时就是一次的变化量为多少。

## 特征转换

在SVM中有些特征模型是没有办法一分为二的，这个时候就需要利用适当的转换将它们转换为可以一刀两断的东西。

比如想要将红点和蓝点变成线性可分的，那么就将映射$$y=x$$变成映射$$y=x^2$$。

![在这里插入图片描述](E:\学习资料\课外小芝士\科研训练\cs231n笔记\图片\20190303081111808.png)

![在这里插入图片描述](E:\学习资料\课外小芝士\科研训练\cs231n笔记\图片\20190303081101648.png)

 这样就线性可分了。







