# lecture 5：卷积神经网络(convolution netural network,convnet)

**全连接**：在最基本的神经网络模型中，我们将三维的图像输入来变成一个一维的向量，然后再通过这个向量进行运算，这样的方式其实是将每一个数据项都与某一个映射关系进行运算。即全连接。

**输入转化为向量。**

**保留输入最基本的形式。**

### 卷积层：

与传统最基本的神经网络相比，使用卷积层时我们在处理图像的输入时不必将输入转换为一维向量，而是直接输入三维的矩阵。然后采用一个指定大小的卷积核来在输入上进行滑动进行矩阵的相乘得到输出。

#### 卷积核（filter）

注意：卷积核负责划定范围，设定参数和对齐。但是最后的本质是深度与深度之间的一维向量间的运算，所以导致在每一个输入点`i行j列`的输出为一个一维的标量，即最后一组卷积核对应的输出为一个二维矩阵(x\*y\*1)。

![image-20230718213309605](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230718213309605.png)

卷积核有关的细节：在卷积核的参数设定上，我们需要关心的有三项：

**卷积核的空间维度(F)**：说人话就是它的大小，到底几乘几。

**步长(stride)**：每次滑动需要移动的长度。

有多少个卷积核：每一个卷积核最终会得到一个1层的输出，作为权重调整之后的输出值。而多个卷积核可以得到多个这样一层的矩阵输出。

注意：输入层与卷积层最终得到的层为1层

![image-20230718214123419](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230718214123419.png)



![image-20230718214643125](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230718214643125.png)

我们记输入的矩阵的长与宽(这里我们不关心它的深度，最后因为卷积核的深度是根据输入数据的深度进行调整的，他俩肯定一样)为$$N$$，卷积核的维数为$$F$$，步长为$$s$$， 那么我们可以得到最后经过卷积层运算后得到的输出的矩阵的大小为：
$$
Output\ Size=(N-F)/s+1
$$


![image-20230719133958290](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230719133958290.png)

#### 零填充

![image-20230719134925349](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230719134925349.png)

#### 学生提问：为什么要进行0填充？

如果没有0填充的话，原始输入经过一层卷积层运算就会变的少一点。我们的输入数据在经过多层的卷积层神经很显然剩余的数据量太少，我们知道，参数越多的情况下你调好了肯定越准确。0填充通过就是为了解决这个问题而存在的。



#### 在下图例中有多少个参数？

这个问题我一开始思考并尝试解答的过程中可以说把能犯的错误都犯了一遍：

1. 图像的原始输入不能被叫做参数，所以32*32\*3不能加到参数里面去。

2. 5*5的filter太容易迷惑人了，往往会忘记了它的深度。深度肯定回合输入的深度是一样的。所以得记得这个深度。

3. 记得审题，有10个filter，所以计算出每一个的时候要乘10.

4. 回顾一下计算公式：
   $$
   Con\ output=W^T+b
   $$
   还有一个bias b呢，这个是一个比较细节的错误，所以总共有(5\*5\*3+1)*10=760个参数

![image-20230719135644305](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230719135644305.png)

### 池化（pooling） ：通过某些函数映射来简化我们的输出。

最常用的方法：max pooling。~~在我目前的理解中是将原输入划分区域，在区域中的数值根据设立的原则最终保留某个特定的映射。~~

不对，池化与卷积层滑动一样，池化过程在一般卷积过程后。池化（pooling） 的本质，其实就是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行降维压缩，以加快运算速度。

池化过程类似于卷积过程，如上图所示，表示的就是对一个 $$4×4$$ feature map邻域内的值，用一个 2×2 的filter，步长为2进行‘扫描’，选择最大值输出到下一层，这叫做 Max Pooling。

max pooling常用的 s=2 ， F=2 的效果：特征图高度、宽度减半，通道数不变。

采用较多的一种池化过程叫**最大池化（Max Pooling）**，其具体操作过程如下：

![image-20230719150612159](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230719150612159.png)

我们不在深度方向上做池化处理，而是只做平面上的。

![image-20230719141241487](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230719141241487.png)

#### 池化的作用：

（1）保留主要特征的同时减少参数和计算量，防止过拟合。

（2）invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，我认为这个妥协会越来越小。

现在有些网络都开始少用或者不用pooling层了。